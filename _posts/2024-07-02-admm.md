---
layout: "default2"
title: "ADMM en simple"
tags: Opti 
---

# <a name="top"></a>ADMM en simple
<span style="color:grey">
{{ page.date | date: "%b %d, %Y" }}
</span>

***

Durante mi tesis de magíster tuve la oportunidad de trabajar en la solución de problemas inversos utilizando métodos de optimización. De todo lo aprendido, el método del lagrangiano aumentado, como nos referíamos coloquialmente al método ADMM (Alternating Direction Method of Multipliers), fue aquel que me marcó. Su rápida convergencia a pesar de la complejidad de las funciones me ha sido útil en múltiples ocasiones, siendo mi favorito al momento de implementar alguna solución de optimización.

Cuando lo estudié no me fue fácil entenderlo, sobre todo cómo extender su implementación a 3 dimensiones, dado que la teoría en general se escribe a base de optimizar un vector. En este post explicaré de forma súper simplificada y espero entendible cómo implementar ADMM.


# ¿Qué es ADMM? 

ADMM resuelve un problema de dos variables definido como sigue:

\\[  \min_{x,z} \; f(x) + g(z)\\]

sujeto a

\\[Ax + Bz = c\\]

El lagrangiano del problema queda definido como: 

\\[ \mathcal{L}(x, z, s) = f(x) + g(z) + \frac{1}{\mu}(Ax+Bz-c+s)\\]

la receta de solución es: 
1. Resolver el subproblema de x. → \\( x^{k+1} = \text{argmin}_{x} \; \mathcal{L}(x, z^k, s^k)  \\)
2. Resolver el sobproblema de z. → \\( z^{k+1} = \text{argmin}_{z} \; \mathcal{L}(x^{k+1}, z, s^k)  \\)
3. Actualizar la variable auxiliar s. → \\( s^{k+1} = s^k + Ax^{k+1} + Bz^{k+1} - c  \\)
4. Volver a 1.

A esta formulación se le conoce como ADMM con variables duales escaladas [(EE364b, Boyd S.)](https://stanford.edu/~boyd/papers/pdf/admm_slides.pdf).

# Ejemplo

## Reducción de ruido 

Consideremos una imagen \\( b \\), podemos encontrar una versión con reducción de ruido \\(x\\) al solucionar el siguiente problema: 

\\[ \min_{x} \; \frac{1}{2}\left\Vert x - b\right\Vert_{2}^{2} + \lambda \left\Vert \nabla x \right\Vert_1\\]

Donde el primer término corresponde a la consistencia de datos, en este caso bajo la norma L2 asumiendo ruido gaussiano. El segundo término corresponde al regularizador, en este caso de variación total. 

Ingresando la variable auxiliar \\( z = \nabla x\\) el problema se convierte en: 

\\[ \min_{x, z} \;  \frac{1}{2}\left\Vert x - b\right\Vert_{2}^{2} + \lambda \left\Vert z \right\Vert_1\\]

sujeto a 
\\[ \nabla x = z\\]

De esta forma logramos la forma de problema dual que resuelve ADMM. El lagrangiano del problema queda definido como: 
\\[ \min_{x,z} \; \frac{1}{2}\left\Vert x - b\right\Vert_{2}^{2} + \lambda\left\Vert z \right\Vert_1 + \frac{\mu_{1}}{2} \left\Vert \nabla x - z + s \right\Vert_{2}^{2}\\]

### Operador gradiente

El operador de gradiente discreto está definido como: 

\\[ \nabla (x[n]) = x[n] - x[n-1]\\]

Para facilitar la derivación en búsqueda de formas cerradas, utilizaremos el operador del gradiente en el dominio de frecuencia utilizando la transformada de fourier. 

\\[ \mathcal{F}\\{x[n]-x[n-1]\\} = \mathcal{F}\\{x[n]\\} - \mathcal{F}\\{x[n-1]\\} = X[n] - e^{\frac{i2\pi}{N}k}X[n] = \underset{E}{\underbrace{(1 - e^{\frac{i2\pi}{N}k})}} X[n] \\]

Por lo que el operador gradiente queda definido como:

\\[ \nabla=F^HEF \\]

donde \\(F\\) es el operador de la transformada de fourier. 

### Subproblema en x 

\\[ \text{argmin}\_{x} \; \frac{1}{2} \left\Vert x - b\right\Vert_{2}^{2} + \frac{\mu_{1}}{2} \left\Vert F^HEFx - z + s \right\Vert_{2}^{2} \\]

Derivamos respecto a \\(x\\) e igualamos a cero.

\\[ x-b + \mu_1F^HE^HF(F^HEFx-z+s) = 0 \\]
\\[ \boxed{x = F^H\frac{Fb+\mu_1E^HF(z-s)}{1+\mu_1E^HE} }\\]

### Subproblema en z

\\[ \text{argmin}\_{z} \;  \lambda\left\Vert z \right\Vert_1 + \frac{\mu_{1}}{2} \left\Vert F^HEFx - z + s \right\Vert_{2}^{2} \\]

Este problema se resuelve mediante un umbral suave (soft thresholding), de la siguiente forma:

Derivamos respecto a \\(z\\) e igualamos a cero.

\\[ \lambda\,\text{sign}(z) - \mu_1(F^HEFx-z+s) = 0\\]

En el óptimo se cumple:
\\[ z = (F^HEFx+s) - \frac{\lambda}{\mu_1}\,\text{sign}(z)\\]
\\[  \forall z < 0, \;  (F^HEFx+s) < -\frac{\lambda}{\mu_1} \; \wedge \; \forall z > 0, \;  (F^HEFx+s) > \frac{\lambda}{\mu_1} \Rightarrow  \left| F^HEFx+s \right| >  \frac{\lambda}{\mu_1}  \Longrightarrow \text{sign}(F^HEFx+s)=\text{sign}(z)\\]

Entonces, 

\\[ z = (F^HEFx+s) - \frac{\lambda}{\mu_1}\,\text{sign}(F^HEFx+s)\\]

Ya analizamos dos condiciones de \\(z\\), solo nos queda el caso \\(z = 0\\)

\\[ z=0  \Rightarrow (F^HEFx+s) \in \left\\{-\frac{\lambda}{\mu_1}, \frac{\lambda}{\mu_1} \right\\}\\]

Entonces la condición de optimalidad es: 

\\[ 0 = (F^HEFx+s) - \frac{\lambda}{\mu_1}\,\text{sign}(F^HEFx+s) \Longleftrightarrow  (F^HEFx+s) \in \left\\{-\frac{\lambda}{\mu_1}, \frac{\lambda}{\mu_1} \right\\}  \Longrightarrow \left\| F^HEFx+s \right\| \leq \left\| \frac{\lambda}{\mu_1} \right\| \\]

Finalmente, 

\\[ \boxed{z = \max\left\( \left\| F^HEFx+s \right\| - \frac{\lambda}{\mu_1}, 0 \right\) \text{sign}(F^HEFx+s)} \\]


### Implementación 

```python
def denoising(im, lamb=1, mu1=1, iter=100):
    """
    Realiza la denoising de una imagen utilizando el método de Alternating Direction Method of Multipliers (ADMM).

    Parámetros:
    ----------
    im : numpy.ndarray
        La imagen de entrada a ser procesada. Debe ser un array 2D.
    lamb : float, opcional
        Parámetro de regularización lambda. Por defecto es 1.
    mu1 : float, opcional
        Parámetro mu1 que controla la penalización. Por defecto es 1.
    iter : int, opcional
        Número de iteraciones del algoritmo. Por defecto es 100.

    Retorna:
    -------
    numpy.ndarray
        La imagen procesada después de aplicar denoising.
    """


    ky = np.arange(im.shape[1]) / im.shape[1]
    kx = np.arange(im.shape[0]) / im.shape[0]

    X, Y = np.meshgrid(ky, kx)

    E_x = 1 - np.exp(2j * np.pi * X)
    E_y = 1 - np.exp(2j * np.pi * Y)

    z_x = np.zeros_like(im)
    z_y = np.zeros_like(im)

    s_x = np.zeros_like(im)
    s_y = np.zeros_like(im)

    x = np.zeros_like(im)

    EEt = E_x*np.conj(E_x) + E_y*np.conj(E_y) 

    for it in range(iter):
        x_prev = x

        t_x = np.conj(E_x) * np.fft.fftn(z_x - s_x)
        t_y = np.conj(E_y) * np.fft.fftn(z_y - s_y)

        x = np.real(
                np.fft.ifftn(
                    (np.fft.fftn(im) + mu1 * (t_x + t_y)) / (
                    1 + mu1 * EEt + np.finfo('float').eps)))

        update = 100 * np.linalg.norm(x - x_prev) / np.linalg.norm(x)
        print(f'iter {it}: {update}')

        if it == iter-1:
            break

        fx = np.fft.fft2(x)
        dx_x = np.real(np.fft.ifftn(E_x * fx))
        dx_y = np.real(np.fft.ifftn(E_y * fx))

        z_x = np.maximum(np.abs(dx_x+s_x) - lamb/mu1, 0) * np.sign(dx_x + s_x)
        z_y = np.maximum(np.abs(dx_y+s_y) - lamb/mu1, 0) * np.sign(dx_y + s_y)

        s_x += dx_x - z_x
        s_y += dx_y - z_y


    return x
```

<div style="text-align: center;">
<img src="/assets/post_imgs/admm/denoising.png" alt="Resultados, parametros: mu1=0.25920270810880663, lamb=0.13442912600008994" style="width: 90%;">
</div>